# Python 

## 数据类型及其相关操作的时间复杂度
### list
|操作|案例|类型|备注|
|:--:|:--:|:--:|:--:|
|index|l[i]|O(1)| |
|store|l[i]=0|O(1)| |
|lenght|len()|O(1)| |
|append|append()|O(1)| |
|pop|pop()|O(1)| |
|clear|clear()\| l=[]|O(1)| |
|clice|l[a:b]|O(b-a)| |
|extend|extend(...)|O(len(...))|取决于长度 |
|construction|list(...)|O(len(...))| 取决于长度|
|insert|insert()|O(1)| |
|pop(i)|pop(i)|O(N)| |
|check|l1==l2|O(N)| |
|sort|sort()|O(NlogN)| |
|multiply|l*k|O(kN)| |


## 进程、线程、协程

进程是具有独立功能的程序，是在某些数据资源上的运行活动，是系统进行资源分配和调度的一个独立单位，是内存分配的最小单位，每个进程都有自己独立的运行空间，不同进程之间通过进程通信来通信。

线程是进程的一个实体，是CPU调度和分配资源的基本单位，线程基本上不拥有系统资源，但可以与同属一个进程的其他线程共享进程的全部资源，线程之间的通信主要通过共享内存来实现，所以上下文切换非常快。

协程是一种轻量级的线程，协程拥有自己的上下文和栈。协程调度切换时，将寄存器上下文和栈保存到其他地方，再切回来的时候，恢复先前保存的寄存器上下文和栈

<font color=red>线程进程都是同步机制，而协程则是异步</font>

## Python scrapy爬虫

- 调度器调度request请求-->引擎-->下载中间件-->下载器
- 下载器发送请求，获取响应-->下载中间件-->引擎-->爬虫中间件-->爬虫
- 爬虫提取URL，封装request请求-->爬虫中间件-->引擎-->调度器
- 提取数据-->引擎-->管道
- 管道进行数据处理和保存

# 机器学习算法

## 逻辑回归
表示形式
$$
y = \frac{1}{1+e^{-(w^T x+b)}}
$$
设
$$
P(Y=1|x)=p(x) \\
P(Y=0|x)=1-p(x)
$$
我们希望当$Y=1$时，p越大越好；当$Y=0$时，p越小越好\
而当$Y=1$时，$-log(p)$刚好符合；当$Y=0$时，$-log(1-p)$刚好符合\
用数学工具将两者结结合，我们就得到了单个样本的损失
$$
J(p, y) = -log(p)^y-log(1-p)^{(1-y)}
$$
对于所有样本的损失则有
$$
\begin{aligned}
J(\theta) &= -\frac{1}{m}\sum_{i=1}^m [y_i log(p_i)+(1-y_i)log(1-p_i)] \\
&=-\frac{1}{m}\sum_{i=1}^m [y_i(log(\frac{p_i}{1-p_i}))+log(1-p_i)]
\end{aligned}
$$
其中
$$
p_i = \frac{1}{1+e^{-\theta^T x_i}}, \quad 1-p_i= \frac{e^{-\theta^T x_i}}{1+e^{-\theta^T x_i}}=\frac{1}{1+e^{\theta^T x_i}}\\
ln\frac{p_i}{1-p_i} = -lne^{-\theta^T x_i}=\theta^Tx_i
$$
将对数都视为自然对数则有
$$
J(\theta) = -\frac{1}{m}\sum_{i=1}^m[ y_i\theta^Tx_i-ln(1+e^{\theta^T x_i})]
$$

对损失 $J(\theta)$求导则有
$$
\begin{aligned}
\nabla  J(\theta) &= -\frac{1}{m}\sum_{i=1}^m (y_i x_i-\frac{e^{\theta^Tx_i}x_i}{1+e^{\theta^T x_i}}) \\
&=-\frac{1}{m}\sum_{i=1}^m (y_i-p)x_i
\end{aligned}
$$


## k-means

- 改进
  - 1、初始点尽可能远
  - 2、每次只用一个子集重入类
  - 3、使用ISODATA确定K
  - 4、将数据映射到高维空间再聚类 
- 聚类异常值检测和处理
  - 使用密度聚类或其他方式聚类，剔除异常值
  - 局部异常因子LOF检测
  - 多远高斯分布异常点检测
  - 使用PCA或自编码器进行异常点检测
  - isolation forest算法
  - winsorize处理（用分位数代替）
- 评价指标
  - 外部法（有标注）：Jaccard系数、纯度
  - 内部法（无标准）：内平方和（wss）、外平方和（bss）
  - 时间空间复杂度
- 补充
  - ISODATA算法参数
    - $K=$ 预期聚类中心数目
    - $\theta_N=$ 每类中最少的样本数，少于这个数则舍弃该类
    - $\theta_S=$ 每类中的标准差界限，大于此值则分裂
    - $\theta_c=$ 两聚类中心的最小距离，小于此值则合并两类
    - $L=$ 一次迭代运算中最多可合并的类数
    - $I=$ 迭代次数
  - ISODATA算法步骤
    - step1. 选择N个初始点
    - step2. 将样本分配给最近的聚类中心
    - step3. 判断每个类的样本数是否大于$\theta_N$
    - step4. 均值算法重新计算类心
    - step5. 计算每类中样本点与类心的**平均距离**
    - step6. 计算**加权平均距离**为总平均距离
    - step7. 计算每类的标准差向量
    - step8. 奇数次迭代进入分裂步骤，偶数次迭代和最后一次迭代进入合并步骤
    - **step9分裂**. 每类的标准差向量中的最大分量的数值$\sigma$，大于$\theta_S$则分裂，类心向量对应分量的值$\pm0.5\sigma$,成为新的类心向量
    - **step9合并**. 计算类心与类心之间的距离，小于$\theta_c$则合并
    - step10. 重复迭代直到达到迭代次数
  - LOF检测
    - LOF算法是通过比较每个点p和邻域点的密度来判断该点是否为异常：点p的密度越低，越有可能是异常点。而点的密度是通过点之间的距离来计算的，点之间距离越远，密度越低；距离越近，密度越高。
    - 特点
      - 非监督算法
      - 基于密度的算法
      - 适合对密度差异较大的数据的异常值检测
  - Jaccard系数
    - 使用来计算两个对象相似度的方法
    - 给定两个n维向量，则Jaccard系数定义如下：
$$
J(A,B)=\frac{\sum_{i}min(x_i,y_i)}{\sum_{i}max(x_i,y_i)}
$$
  - 纯度
    - 纯度和准确率其实是一样的
    - 纯度的计算方式为每类正确分类的样本数量和除以总的样本数量
  - 多元高斯分布
    - 先训练正样本，得出正样本的概率密度函数，设置阈值，异常样本的的概率低于该值时判断为异常样本
    - 
        | 普通高斯分布 | 多元高斯分布 |
        | ------ | ------ |
        | 运算量小 | 能够发现不同特征之间的相关性 |
        | 特征之间独立时使用较快 | m=10n 且协方差矩阵可逆时使用 |
  - **isolation forest算法**
    - 相较于LOF，K-means等传统算法，孤立森林算法对高纬数据有较好的鲁棒性。
    - 第一阶段，步骤如下：

      - （1）从训练数据中随机选择Ψ个点样本点作为样本子集，放入树的根节点。

      - （2）随机指定一个维度（特征），在当前节点数据中随机产生一个切割点 p（切割点产生于当前节点数据中指定维度的最大值和最小值之间）。

      - （3）以此切割点生成了一个超平面，然后将当前节点数据空间划分为2个子空间：把指定维度里小于 p 的数据放在当前节点的左子节点，把大于等于 p 的数据放在当前节点的右子节点。

      - （4）在子节点中递归步骤（2）和（3），不断构造新的孩子节点，直到子节点中只有一个数据（无法再继续切割）或子节点已到达限定高度。

      - （5）循环（1）至（4），直至生成 t 个孤立树iTree。

    - 第二阶段：

      - 获得t个iTree之后，iForest 训练就结束，然后我们可以用生成的iForest来评估测试数据了。对于每一个数据点 xi，令其遍历每一颗孤立树（iTree），计算点 &x_i& 在森林中的平均高度好$h(x_i)$，对所有点的平均高度做归一化处理。
    - 特点：
      - 具有线性时间复杂度，可以用在海量数据集上面，一般输的数量越多，算法越稳定。
      - 不太适合高纬度数据。
      - 对全局稀疏点敏感，不擅长处理局部的相对稀疏点。

## SVM
- SVM的优点：
  - 能应用于线性不可分的情况
  - 复杂度由样本数量决定而不是数据的维度
  - 具有鲁棒性，只使用少量支持向量，抓住关键信息
  - 高维地样本数量下表现较好
- SVM的缺点：
  - 训练复杂度较高
  - 难以适应多分类问题
  - 核函数的选择没有较好的方法论


## 决策树
  - 如何避免决策树过拟合
    - 1.限制树深
    - 2.剪枝
    - 3.限制叶子节点的数量
    - 4.正则化
    - 5.增加数据
    - 6.bagging
    - 7.数据增强
    - 8.早停

## GBDT
梯度提升决策树（Gradient Boosting Decision Tree，GBDT）是一种基于boosting的集成学习算法，训练时采用**前向分布算法**进行贪婪学习，每次迭代都学习一颗cart树来拟合之前t-1棵树的残差。
对于训练数据 $T={(x_1, y_1), (x_2, y_2), \cdots (x_i, y_i)}, x_i\in X\subseteq R^n,y_i\in Y\subseteq R$；损失函数为 $L(y, f(x))$
初始化学习器
$$
f_0(x)=\arg \min_c \sum_i^NL(y_i,c)
$$
对c求导可得初始化时c的取值
$$
\begin{aligned}
\frac{\partial f_0(x)}{\partial c} = &\sum_i^n \frac{\partial L(y_i, c)}{\partial c}\\
=&\sum_i^n\frac{\partial (\frac{1}{2}(y_i-c)^2)}{\partial c} \\
=&\sum_i^n (c-y_i)
\end{aligned}
$$
令导数等于0可得
$$
c = \frac{1}{N}\sum_i^n y_i
$$
所以初始化学习器的c为全部样本训练标签的均值，则有
$$
f_0(x)=c
$$
迭代训练 $m=1,2,\cdots,M$棵树的过程如下\
对每个样本计算负梯度
$$
r_{mi}=-\frac{\partial L(y_i,f(x_i))}{\partial f(x_i)},\qquad其中f(x)=f_{m-1}(x)
$$
负梯度在这里就是残差(当损失函数不为平方损失时，负梯度近似残差)，于是有
$$
r_{mj} = \arg \min_c \sum_{x_i\in R_{mj}} L(y_i, f_{m-1}(x_i)+c) \\
f_m(x) = f_{m-1}(x)+\sum_{j=1}^J c_{mj} I(x\in R_{mj})
$$
其中 $I(x\in R_{mj})$为示性函数，若 $x\in R_{mj}$，则 $I=1$，否则 $I=0$

所以树加起来，最终得到学习器
$$
f_M(x) = f_0(x)+\sum_{m=1}^M\sum_{j=1}^J c_{mj} I(x\in R_{mj})
$$


## XGBoost






# B端数据分析思路

- 分析场景
  - 面向市场的竞品分析
  - 面向客户的运营分析
  - 面向产品的用研分析
  - 面向管理的经营分析
  - 面向质量的问题分析

- 算法场景
  - 基于标签的推荐算法：根据标签、用户偏好、商品信息建立混淆矩阵，生成差异化的规则和策略
  - 基于商品的分类预测：主要从商品的类目、品牌、型号、规格、属性等特征，建立分类模型
  - 基于交易的趋势预测：主要从市场大盘、行业趋势、单品交易、经营策略等维度建立预测模型
  - 基于市场的供需模型：主要从供给频次、需求量、供给量、供应周期等方面入手
  - 基于用户的分析：维度有渠道来源、访问频次、浏览量、点击、加购、收藏、支付方式
  - 基于风险的风控模型：需要考虑的点有履约、违约、诉讼、行政处罚、刑事处罚、经营状况
  - 基于客服的语义模型：客户诉求、电话咨询、机器人问答、人工客服等场景

# 统计学

## 一类错误二类错误
- 一类错误：弃真，即当原假设为真时，做出拒绝原假设的判断
- 二类错误：取伪，即当原假设为假时，做出接受原假设的判断
从统计上来讲，犯一类错误的概率由显著性水平所决定，提高显著性水平会降低犯一类错误的概率，但会提高犯二类错误的概率 \
当样本量增大时，样本观察值的方差减小，样本观察值会越向均值靠拢，因此样本观察值的概率密度曲线会变得更加“瘦高”，这也意味着阴影部分面积减小，即二类错误概率减小，统计功效增大



# A-B test

# 因果推断

## CUPED





























